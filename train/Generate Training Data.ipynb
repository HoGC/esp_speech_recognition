{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare audio data for image recognition\n",
    "\n",
    "The data is pretty good, but there's a few samples that aren't exactly 1 second long and some samples that are either truncated or don't contain very much of the word.\n",
    "\n",
    "The code in the notebook attempts to filter out the broken audio so that we are only using good audio.\n",
    "\n",
    "We then generate spectrograms of each word. We mix in background noise with the words to make it a more realistic audio sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data set\n",
    "Download from: https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz - approx 2.3 GB\n",
    "\n",
    "And then run\n",
    "```\n",
    "tar -xzf data_speech_commands_v0.02.tar.gz -C speech_data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir speech_data\n",
    "!tar -xzf data_speech_commands_v0.02.tar.gz -C speech_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow tensorflow_io numpy tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.io import gfile\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.python.ops import gen_audio_ops as audio_ops\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 使用CPU\n",
    "cpus = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
    "tf.config.experimental.set_visible_devices(devices=cpus[0], device_type='CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEECH_DATA='speech_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The audio is all sampled at 16KHz and should all be 1 second in length - so 1 second is 16000 samples\n",
    "EXPECTED_SAMPLES=16000\n",
    "# Noise floor to detect if any audio is present\n",
    "NOISE_FLOOR=0.1\n",
    "# How many samples should be abover the noise floor?\n",
    "MINIMUM_VOICE_LENGTH=EXPECTED_SAMPLES/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of folders we want to process in the speech_data folder\n",
    "from tensorflow.python.ops import gen_audio_ops as audio_ops\n",
    "words = [\n",
    "    'backward',\n",
    "    'bed',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'dog',\n",
    "    'down',\n",
    "    'eight',\n",
    "    'five',\n",
    "    'follow',\n",
    "    'forward',\n",
    "    'four',\n",
    "    'go',\n",
    "    'happy',\n",
    "    'house',\n",
    "    'learn',\n",
    "    'left',\n",
    "    'marvin',\n",
    "    'nine',\n",
    "    'no',\n",
    "    'off',\n",
    "    'on',\n",
    "    'one',\n",
    "    'right',\n",
    "    'seven',\n",
    "    'sheila',\n",
    "    'six',\n",
    "    'stop',\n",
    "    'three',\n",
    "    'tree',\n",
    "    'two',\n",
    "    'up',\n",
    "    'visual',\n",
    "    'wow',\n",
    "    'yes',\n",
    "    'zero',\n",
    "    '_background',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the files in a directory\n",
    "def get_files(word):\n",
    "    return gfile.glob(SPEECH_DATA + '/'+word+'/*.wav')\n",
    "\n",
    "# get the location of the voice\n",
    "def get_voice_position(audio, noise_floor):\n",
    "    audio = audio - np.mean(audio)\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    return tfio.audio.trim(audio, axis=0, epsilon=noise_floor)\n",
    "\n",
    "# Work out how much of the audio file is actually voice\n",
    "def get_voice_length(audio, noise_floor):\n",
    "    position = get_voice_position(audio, noise_floor)\n",
    "    return (position[1] - position[0]).numpy()\n",
    "\n",
    "# is enough voice present?\n",
    "def is_voice_present(audio, noise_floor, required_length):\n",
    "    voice_length = get_voice_length(audio, noise_floor)\n",
    "    return voice_length >= required_length\n",
    "\n",
    "# is the audio the correct length?\n",
    "def is_correct_length(audio, expected_length):\n",
    "    return (audio.shape[0]==expected_length).numpy()\n",
    "\n",
    "\n",
    "def is_valid_file(file_name):\n",
    "    # load the audio file\n",
    "    audio_tensor = tfio.audio.AudioIOTensor(file_name)\n",
    "    # check the file is long enough\n",
    "    if not is_correct_length(audio_tensor, EXPECTED_SAMPLES):\n",
    "        return False\n",
    "    # convert the audio to an array of floats and scale it to betweem -1 and 1\n",
    "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
    "    audio = audio - np.mean(audio)\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    # is there any voice in the audio?\n",
    "    if not is_voice_present(audio, NOISE_FLOOR, MINIMUM_VOICE_LENGTH):\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(audio):\n",
    "    # normalise the audio\n",
    "    audio = audio - np.mean(audio)\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    # create the spectrogram\n",
    "    spectrogram = audio_ops.audio_spectrogram(audio,\n",
    "                                              window_size=320,\n",
    "                                              stride=160,\n",
    "                                              magnitude_squared=True).numpy()\n",
    "    # reduce the number of frequency bins in our spectrogram to a more sensible level\n",
    "    spectrogram = tf.nn.pool(\n",
    "        input=tf.expand_dims(spectrogram, -1),\n",
    "        window_shape=[1, 6],\n",
    "        strides=[1, 6],\n",
    "        pooling_type='AVG',\n",
    "        padding='SAME')\n",
    "    spectrogram = tf.squeeze(spectrogram, axis=0)\n",
    "    spectrogram = np.log10(spectrogram + 1e-6)\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process a file into its spectrogram\n",
    "def process_file(file_path):\n",
    "    # load the audio file\n",
    "    audio_tensor = tfio.audio.AudioIOTensor(file_path)\n",
    "    # convert the audio to an array of floats and scale it to betweem -1 and 1\n",
    "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
    "    audio = audio - np.mean(audio)\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    # randomly reposition the audio in the sample\n",
    "    voice_start, voice_end = get_voice_position(audio, NOISE_FLOOR)\n",
    "    end_gap=len(audio) - voice_end\n",
    "    random_offset = np.random.uniform(0, voice_start+end_gap)\n",
    "    audio = np.roll(audio,-random_offset+end_gap)\n",
    "    # add some random background noise\n",
    "    background_volume = np.random.uniform(0, 0.1)\n",
    "    # get the background noise files\n",
    "    background_files = get_files('_background_noise_')\n",
    "    background_file = np.random.choice(background_files)\n",
    "    background_tensor = tfio.audio.AudioIOTensor(background_file)\n",
    "    background_start = np.random.randint(0, len(background_tensor) - 16000)\n",
    "    # normalise the background noise\n",
    "    background = tf.cast(background_tensor[background_start:background_start+16000], tf.float32)\n",
    "    background = background - np.mean(background)\n",
    "    background = background / np.max(np.abs(background))\n",
    "    # mix the audio with the scaled background\n",
    "    audio = audio + background_volume * background\n",
    "    # get the spectrogram\n",
    "    return get_spectrogram(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "validate = []\n",
    "test = []\n",
    "\n",
    "TRAIN_SIZE=0.8\n",
    "VALIDATION_SIZE=0.1\n",
    "TEST_SIZE=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(file_names, label, repeat=1):\n",
    "    file_names = tf.repeat(file_names, repeat).numpy()\n",
    "    return [(process_file(file_name), label) for file_name in tqdm(file_names, desc=f\"{word} ({label})\", leave=False)]\n",
    "\n",
    "# process the files for a word into the spectrogram and one hot encoding word value\n",
    "def process_word(word, repeat=1):\n",
    "    # the index of the word word we are processing\n",
    "    label = words.index(word)\n",
    "    # get a list of files names for the word\n",
    "    file_names = [file_name for file_name in tqdm(get_files(word), desc=\"Checking\", leave=False) if is_valid_file(file_name)]\n",
    "    # randomly shuffle the filenames\n",
    "    np.random.shuffle(file_names)\n",
    "    # split the files into train, validate and test buckets\n",
    "    train_size=int(TRAIN_SIZE*len(file_names))\n",
    "    validation_size=int(VALIDATION_SIZE*len(file_names))\n",
    "    test_size=int(TEST_SIZE*len(file_names))\n",
    "    # get the training samples\n",
    "    train.extend(\n",
    "        process_files(\n",
    "            file_names[:train_size],\n",
    "            label,\n",
    "            repeat=repeat\n",
    "        )\n",
    "    )\n",
    "    # and the validation samples\n",
    "    validate.extend(\n",
    "        process_files(\n",
    "            file_names[train_size:train_size+validation_size],\n",
    "            label,\n",
    "            repeat=repeat\n",
    "        )\n",
    "    )\n",
    "    # and the test samples\n",
    "    test.extend(\n",
    "        process_files(\n",
    "            file_names[train_size+validation_size:],\n",
    "            label,\n",
    "            repeat=repeat\n",
    "        )\n",
    "    )\n",
    "\n",
    "# process all the words and all the files\n",
    "for word in tqdm(words, desc=\"Processing words\"):\n",
    "    if '_' not in word:\n",
    "        # add more examples of marvin to balance our training set\n",
    "        repeat = 70 if word == 'marvin' else 1\n",
    "        process_word(word, repeat=repeat)\n",
    "    \n",
    "print(len(train), len(test), len(validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the background noise files\n",
    "def process_background(file_name, label):\n",
    "    # load the audio file\n",
    "    audio_tensor = tfio.audio.AudioIOTensor(file_name)\n",
    "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
    "    audio_length = len(audio)\n",
    "    samples = []\n",
    "    for section_start in tqdm(range(0, audio_length-EXPECTED_SAMPLES, 8000), desc=file_name, leave=False):\n",
    "        section_end = section_start + EXPECTED_SAMPLES\n",
    "        section = audio[section_start:section_end]\n",
    "        # get the spectrogram\n",
    "        spectrogram = get_spectrogram(section)\n",
    "        samples.append((spectrogram, label))\n",
    "\n",
    "    # simulate random utterances\n",
    "    for section_index in tqdm(range(1000), desc=\"Simulated Words\", leave=False):\n",
    "        section_start = np.random.randint(0, audio_length - EXPECTED_SAMPLES)\n",
    "        section_end = section_start + EXPECTED_SAMPLES\n",
    "        section = np.reshape(audio[section_start:section_end], (EXPECTED_SAMPLES))\n",
    "\n",
    "        result = np.zeros((EXPECTED_SAMPLES))\n",
    "        # create a pseudo bit of voice\n",
    "        voice_length = np.random.randint(MINIMUM_VOICE_LENGTH/2, EXPECTED_SAMPLES)\n",
    "        voice_start = np.random.randint(0, EXPECTED_SAMPLES - voice_length)\n",
    "        hamming = np.hamming(voice_length)\n",
    "        # amplify the voice section\n",
    "        result[voice_start:voice_start+voice_length] = hamming * section[voice_start:voice_start+voice_length]\n",
    "        # get the spectrogram\n",
    "        spectrogram = get_spectrogram(np.reshape(section, (16000, 1)))\n",
    "        samples.append((spectrogram, label))\n",
    "        \n",
    "    \n",
    "    np.random.shuffle(samples)\n",
    "    \n",
    "    train_size=int(TRAIN_SIZE*len(samples))\n",
    "    validation_size=int(VALIDATION_SIZE*len(samples))\n",
    "    test_size=int(TEST_SIZE*len(samples))\n",
    "    \n",
    "    train.extend(samples[:train_size])\n",
    "\n",
    "    validate.extend(samples[train_size:train_size+validation_size])\n",
    "\n",
    "    test.extend(samples[train_size+validation_size:])\n",
    "\n",
    "        \n",
    "for file_name in tqdm(get_files('_background_noise_'), desc=\"Processing Background Noise\"):\n",
    "    process_background(file_name, words.index(\"_background\"))\n",
    "    \n",
    "print(len(train), len(test), len(validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_problem_noise(file_name, label):\n",
    "    samples = []\n",
    "    # load the audio file\n",
    "    audio_tensor = tfio.audio.AudioIOTensor(file_name)\n",
    "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
    "    audio_length = len(audio)\n",
    "    samples = []\n",
    "    for section_start in tqdm(range(0, audio_length-EXPECTED_SAMPLES, 400), desc=file_name, leave=False):\n",
    "        section_end = section_start + EXPECTED_SAMPLES\n",
    "        section = audio[section_start:section_end]\n",
    "        # get the spectrogram\n",
    "        spectrogram = get_spectrogram(section)\n",
    "        samples.append((spectrogram, label))\n",
    "        \n",
    "    np.random.shuffle(samples)\n",
    "    \n",
    "    train_size=int(TRAIN_SIZE*len(samples))\n",
    "    validation_size=int(VALIDATION_SIZE*len(samples))\n",
    "    test_size=int(TEST_SIZE*len(samples))\n",
    "    \n",
    "    train.extend(samples[:train_size])\n",
    "    validate.extend(samples[train_size:train_size+validation_size])\n",
    "    test.extend(samples[train_size+validation_size:])\n",
    "\n",
    "\n",
    "for file_name in tqdm(get_files(\"_problem_noise_\"), desc=\"Processing problem noise\"):\n",
    "    process_problem_noise(file_name, words.index(\"_background\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mar_sounds(file_name, label):\n",
    "    samples = []\n",
    "    # load the audio file\n",
    "    audio_tensor = tfio.audio.AudioIOTensor(file_name)\n",
    "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
    "    audio_length = len(audio)\n",
    "    samples = []\n",
    "    for section_start in tqdm(range(0, audio_length-EXPECTED_SAMPLES, 4000), desc=file_name, leave=False):\n",
    "        section_end = section_start + EXPECTED_SAMPLES\n",
    "        section = audio[section_start:section_end]\n",
    "        section = section - np.mean(section)\n",
    "        section = section / np.max(np.abs(section))\n",
    "        # add some random background noise\n",
    "        background_volume = np.random.uniform(0, 0.1)\n",
    "        # get the background noise files\n",
    "        background_files = get_files('_background_noise_')\n",
    "        background_file = np.random.choice(background_files)\n",
    "        background_tensor = tfio.audio.AudioIOTensor(background_file)\n",
    "        background_start = np.random.randint(0, len(background_tensor) - 16000)\n",
    "        # normalise the background noise\n",
    "        background = tf.cast(background_tensor[background_start:background_start+16000], tf.float32)\n",
    "        background = background - np.mean(background)\n",
    "        background = background / np.max(np.abs(background))\n",
    "        # mix the audio with the scaled background\n",
    "        section = section + background_volume * background\n",
    "        # get the spectrogram\n",
    "        spectrogram = get_spectrogram(section)\n",
    "        samples.append((spectrogram, label))\n",
    "        \n",
    "    np.random.shuffle(samples)\n",
    "    \n",
    "    train_size=int(TRAIN_SIZE*len(samples))\n",
    "    validation_size=int(VALIDATION_SIZE*len(samples))\n",
    "    test_size=int(TEST_SIZE*len(samples))\n",
    "    \n",
    "    train.extend(samples[:train_size])\n",
    "    validate.extend(samples[train_size:train_size+validation_size])\n",
    "    test.extend(samples[train_size+validation_size:])\n",
    "\n",
    "\n",
    "for file_name in tqdm(get_files(\"_mar_sounds_\"), desc=\"Processing problem noise\"):\n",
    "    process_mar_sounds(file_name, words.index(\"_background\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(train), len(test), len(validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomise the training samples\n",
    "np.random.shuffle(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = zip(*train)\n",
    "X_validate, Y_validate = zip(*validate)\n",
    "X_test, Y_test = zip(*test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the computed data\n",
    "np.savez_compressed(\n",
    "    \"training_spectrogram.npz\",\n",
    "    X=X_train, Y=Y_train)\n",
    "print(\"Saved training data\")\n",
    "np.savez_compressed(\n",
    "    \"validation_spectrogram.npz\",\n",
    "    X=X_validate, Y=Y_validate)\n",
    "print(\"Saved validation data\")\n",
    "np.savez_compressed(\n",
    "    \"test_spectrogram.npz\",\n",
    "    X=X_test, Y=Y_test)\n",
    "print(\"Saved test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the width and height of the spectrogram \"image\"\n",
    "IMG_WIDTH=X_train[0].shape[0]\n",
    "IMG_HEIGHT=X_train[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images2(images_arr, imageWidth, imageHeight):\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(10, 20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(np.reshape(img, (imageWidth, imageHeight)))\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = words.index(\"marvin\")\n",
    "\n",
    "X_marvins = np.array(X_train)[np.array(Y_train) == word_index]\n",
    "Y_marvins = np.array(Y_train)[np.array(Y_train) == word_index]\n",
    "plot_images2(X_marvins[:20], IMG_WIDTH, IMG_HEIGHT)\n",
    "print(Y_marvins[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = words.index(\"yes\")\n",
    "\n",
    "X_yes = np.array(X_train)[np.array(Y_train) == word_index]\n",
    "Y_yes = np.array(Y_train)[np.array(Y_train) == word_index]\n",
    "plot_images2(X_yes[:20], IMG_WIDTH, IMG_HEIGHT)\n",
    "print(Y_yes[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
